# Linear and Logistic Regression Implementation
My coding homework for Homework 1 of the Spring 2021 offering of CS 446 at UIUC. hw1 and hw1_utils use Pytorch and Matplotlib.pyplot. I wrote all of the code in hw1 except for the docstrings, the function headers, and the function signatures, although I wrote the function header and signature for lin_gd_second_pred(X) and the comment at the top all by myself. The file hw1 contains an algorithm for each of the following: pure linear regression via gradient descent, pure linear regression via the pseudoinverse, polynomial regression (via linear regression) using gradient descent, polynomial regression (via linear regression) using the pseudoinverse, logistic regression, linear classification via linear and logistic regression, and generation of plots for comparisons between pure linear regression and polynomial regression for certain datasets for best-fit line and linear classification as well as for comparisons between pure linear regression and logistic regression for linear classification. Both pure linear and polynomial regression are performed in this file via ordinary least squares.

Note: in logistic_vs_linear_regression_for_linear_classification, the yellow line is the line obtained by linear regression while the green line is the line obtained by logistic regression. Clearly the green line classifies the data better, which shows, empirically, that the definition of logistic loss penalizes whether the predicted output differs from the expected output in sign or is equal to 0 while the definition of linear regression makes no such penalty.
